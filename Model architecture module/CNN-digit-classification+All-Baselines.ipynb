{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "CNN-digit-classification+All-Baselines.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q-1u4YdKNgs"
      },
      "source": [
        "## All baselines\n",
        "import os\n",
        "#from CKA import linear_CKA, kernel_CKA\n",
        "#import cca_core\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split,KFold\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle \n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from tensorflow.keras import backend as K\n",
        "#from tensorflow import set_random_seed\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "#Importing the necessary packages and libaries\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbkDZS3yLM4y",
        "outputId": "a2648806-c78a-4af6-e10f-93706162c059",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tf-nightly"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tf-nightly in /usr/local/lib/python3.7/dist-packages (2.8.0.dev20211022)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.17.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.15.0)\n",
            "Requirement already satisfied: tf-estimator-nightly~=2.8.0.dev in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (2.8.0.dev2021102208)\n",
            "Requirement already satisfied: keras-nightly~=2.8.0.dev in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (2.8.0.dev2021102207)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.12.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.41.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.12)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.21.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: tb-nightly~=2.7.0.a in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (2.7.0a20211013)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.7.4.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (12.0.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.37.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tf-nightly) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf-nightly) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf-nightly) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf-nightly) (3.3.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf-nightly) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf-nightly) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.7.0.a->tf-nightly) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.7.0.a->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.7.0.a->tf-nightly) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.7.0.a->tf-nightly) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.7.0.a->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly~=2.7.0.a->tf-nightly) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly~=2.7.0.a->tf-nightly) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.7.0.a->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.7.0.a->tf-nightly) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.7.0.a->tf-nightly) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.7.0.a->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.7.0.a->tf-nightly) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tb-nightly~=2.7.0.a->tf-nightly) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tupqQD_DKNgy"
      },
      "source": [
        "# Random seed function\n",
        "def reset_random_seeds():\n",
        "    seed_value=1\n",
        "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "    tf.random.set_random_seed(seed_value)\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    set_random_seed(seed_value)\n",
        "    session_conf=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "    sess=tf.Session(graph=tf.get_default_graph(),config=session_conf)\n",
        "    K.set_session(sess)\n",
        "\n",
        "# load data\n",
        "def load_data(dataset_path):\n",
        "    with open(dataset_path,\"r\") as fp:\n",
        "        data=json.load(fp)  #load the dictionary from the file\n",
        "        \n",
        "    # convert list into numpy array\n",
        "    inputs=np.array(data[\"mfcc\"])\n",
        "    targets=np.array(data[\"labels\"]) #.astype(int)\n",
        "    #targets=gender_to_digit(targets)\n",
        "    return inputs,targets\n",
        "\n",
        "def prepare_datasets(test_size,DATASET_PATH,ln):\n",
        "    #load data\n",
        "    X,y=load_data(DATASET_PATH)\n",
        "    if ln==\"sw\":\n",
        "        X,y=sort_dataset(X,y)\n",
        "    \n",
        "    #create train/test split\n",
        "    #split the data into train and test set\n",
        "    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=test_size)\n",
        "    X_train=X_train[...,np.newaxis] #4d array-> (num_samples,130,13,1)\n",
        "    X_test=X_test[...,np.newaxis]\n",
        "        \n",
        "    return X_train,y_train,X_test,y_test\n",
        "# resize Swahili and English datasets size(3000 utterances) to Gujarati size (1940 utterances)\n",
        "def resize_to_gu_size(X,y):\n",
        "    y_l=[]\n",
        "    X_l=[]\n",
        "    for i in range(0,2702,300):\n",
        "        if i!=0:\n",
        "            y_l.append(y[i+1:i+1+194])\n",
        "            X_l.append(X[i+1:i+1+194])\n",
        "        else:\n",
        "            y_l.append(y[i:194])\n",
        "            X_l.append(X[i:194])\n",
        "    \n",
        "    X_l=np.array(X_l)\n",
        "    y_l=np.array(y_l)\n",
        "    X_l=X_l.reshape(1940,64,16)\n",
        "    y_l=y_l.reshape(1940,)\n",
        "    return X_l,y_l\n",
        "\n",
        "# Sort datasets y[0,2,9,0,0,3,....,4,8,1] -> y[0,0,0,1,1,1,2,2,2.....9,9,9] and its corresponding Xs[....] \n",
        "def sort_dataset(X,y):    \n",
        "    #convert into list from numpy\n",
        "    X=list(X)\n",
        "    y=list(y)\n",
        "    #zip\n",
        "    data1 = list(zip(y, X))\n",
        "    #sort multiple list in the zip\n",
        "    data1=sorted(data1, key=lambda x: x[0])\n",
        "    #unzip\n",
        "    y_s, X_s = zip(*data1)\n",
        "    #convert back to numpy from tuple/list\n",
        "    y_s=np.asarray(y_s)\n",
        "    X_s=np.asarray(X_s)\n",
        "    \n",
        "    return X_s,y_s\n",
        "\n",
        "# Section 5.1 \n",
        "# ln -language\n",
        "def prepare_datasets_gu(test_size,DATASET_PATH,ln):\n",
        "    # load data\n",
        "    X,y=load_data(DATASET_PATH)\n",
        "    # re-sort dataset\n",
        "    X,y=sort_dataset(X,y)\n",
        "    if ln==\"swen\": #select language\n",
        "        X,y=resize_to_gu_size(X,y)  #resize to 1940 utterances\n",
        "    \n",
        "    #create train/test split\n",
        "    #split the data into train and test set\n",
        "    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=test_size)\n",
        "    X_train=X_train[...,np.newaxis] #4d array-> (num_samples,130,13,1)\n",
        "    X_test=X_test[...,np.newaxis]\n",
        "        \n",
        "    return X_train,y_train,X_test,y_test\n",
        "\n",
        "def plot_history(history):\n",
        "    fig,axs=plt.subplots(2)\n",
        "    \n",
        "    #create accuracy suplot\n",
        "    axs[0].plot(history.history[\"acc\"],label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_acc\"],label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "    \n",
        "    #create accuracy suplot\n",
        "    axs[1].plot(history.history[\"loss\"],label=\"train error\")\n",
        "    axs[1].plot(history.history[\"val_loss\"],label=\"test error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWgvi5VoKNg0"
      },
      "source": [
        "# Download the MFCCs files from https://drive.google.com/drive/folders/1x7tZIfxsTRTB-hdPx5QSLm27dq1zrx2J?usp=sharing\n",
        "DATASET_PATH_SW= \"MNIST-digit-sw-mfcc-norm-16.json\"  #Swahili dataset file\n",
        "DATASET_PATH_EN= \"MNIST-digit-en-mfcc-norm-16.json\"  #English dataset file\n",
        "DATASET_PATH_GU= \"MNIST-digit-gu-mfcc-norm-16.json\"  #Gurajati dataset file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXelAX5JKNg1"
      },
      "source": [
        "# Load each dataset into its container\n",
        "if __name__==\"__main__\":\n",
        "    #inputs,targets=load_data(DATASET_PATH)\n",
        "    reset_random_seeds()\n",
        "        \n",
        "    #Swahili Train and Test dataset for X features and y labels\n",
        "    X_train_sw,y_train_sw,X_test_sw,y_test_sw=prepare_datasets_gu(0.2,DATASET_PATH_SW,\"swen\")\n",
        "    reset_random_seeds()\n",
        "    \n",
        "    #English Train and Test dataset for X features and y labels\n",
        "    X_train_en,y_train_en,X_test_en,y_test_en=prepare_datasets_gu(0.2,DATASET_PATH_EN,\"swen\")\n",
        "    reset_random_seeds()\n",
        "    \n",
        "    #Gujarati Train and Test dataset for X features and y labels\n",
        "    X_train_gu,y_train_gu,X_test_gu,y_test_gu=prepare_datasets_gu(0.2,DATASET_PATH_GU,\"gu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCZ38oLXKNg2"
      },
      "source": [
        "# Reshape for svm training and evaluating\n",
        "X_train_sw_svm=X_train_sw.reshape(1552, 1024)\n",
        "X_test_sw_svm=X_test_sw.reshape(388,1024)\n",
        "\n",
        "X_train_en_svm=X_train_en.reshape(1552, 1024)\n",
        "X_test_en_svm=X_test_en.reshape(388,1024)\n",
        "\n",
        "X_train_gu_svm=X_train_gu.reshape(1552, 1024)\n",
        "X_test_gu_svm=X_test_gu.reshape(388,1024)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYSspopkKNg2",
        "outputId": "7eb5ce43-6943-4f00-e604-07a528811bfe"
      },
      "source": [
        "X_test_sw.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(388, 64, 16, 1)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK0g_ZI-KNg4",
        "outputId": "49e4ea89-5363-41f6-eac5-e599c5401734"
      },
      "source": [
        "y_train_gu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['3', '0', '3', ..., '9', '1', '5'], dtype='<U1')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBnWKpteKNg5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQNVYdacKNg6",
        "outputId": "1f7ffa53-029c-43e9-b9ea-d303841ca5e6"
      },
      "source": [
        "# Training and Evaluating SVM for kernels linear, RBF, \n",
        "gamma=0.5   \n",
        "for i in range(20):\n",
        "    \n",
        "    \n",
        "    # training Swahili dataset\n",
        "    print(\"C value:\",i+1)  \n",
        "   # linear_sw = svm.SVC(kernel='linear', C=i+1, decision_function_shape='ovo').fit(X_train_sw_svm, y_train_sw)\n",
        "    linear_sw = svm.SVC(kernel='rbf', gamma=gamma, C=i+1, decision_function_shape='ovo').fit(X_train_sw_svm, y_train_sw)\n",
        "  #  linear_sw = svm.SVC(kernel='poly', degree=1, C=i+1, decision_function_shape='ovo').fit(X_train_sw_svm, y_train_sw)\n",
        "    accuracy_lin_sw = linear_sw.score(X_test_sw_svm, y_test_sw)\n",
        "    print(\"Accuracy Linear Kernel Swahili:\", accuracy_lin_sw)\n",
        "\n",
        "    # training English dataset\n",
        "   # linear_en = svm.SVC(kernel='linear', C=i+1, decision_function_shape='ovo').fit(X_train_en_svm, y_train_en)\n",
        "    linear_en = svm.SVC(kernel='rbf', gamma=gamma, C=i+1, decision_function_shape='ovo').fit(X_train_en_svm, y_train_en)\n",
        "   # linear_en = svm.SVC(kernel='poly', degree=1, C=i+1, decision_function_shape='ovo').fit(X_train_en_svm, y_train_en)\n",
        "    accuracy_lin_en = linear_en.score(X_test_en_svm, y_test_en)\n",
        "    print(\"Accuracy Linear Kernel English:\", accuracy_lin_en)\n",
        "\n",
        "    # training Gujarati dataset\n",
        "   # linear_gu = svm.SVC(kernel='linear', C=i+1, decision_function_shape='ovo').fit(X_train_gu_svm, y_train_gu)\n",
        "    linear_gu = svm.SVC(kernel='rbf', gamma=gamma, C=i+1, decision_function_shape='ovo').fit(X_train_gu_svm, y_train_gu)\n",
        "  #  linear_gu = svm.SVC(kernel='poly', degree=1, C=i+1, decision_function_shape='ovo').fit(X_train_gu_svm, y_train_gu)\n",
        "    accuracy_lin_gu = linear_gu.score(X_test_gu_svm, y_test_gu)\n",
        "    print(\"Accuracy Linear Kernel Gujarati:\", accuracy_lin_gu)\n",
        "    print (\"------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C value: 20\n",
            "Accuracy Linear Kernel Swahili: 0.5489690721649485\n",
            "Accuracy Linear Kernel English: 0.6030927835051546\n",
            "Accuracy Linear Kernel Gujarati: 0.5335051546391752\n",
            "------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILCm02LZKNg7",
        "outputId": "dbbff61b-54dd-4e2f-b903-693a1ea217b0"
      },
      "source": [
        "# Training and Evaluating Random Forest\n",
        "no_tree=[10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200]\n",
        "# Fitting Random Forest Classification to the Training set\n",
        "for i in range(len(u)):\n",
        "    print(\"no tree values:\",u[i]) \n",
        "    # Training Swahili\n",
        "    classifier_sw = RandomForestClassifier(n_estimators = no_tree[i], criterion = 'entropy', random_state = 42)\n",
        "    classifier_sw.fit(X_train_sw_svm, y_train_sw)\n",
        "    y_pred_sw = classifier_sw.predict(X_test_sw_svm)\n",
        "    print(\"Accuracy:\",metrics.accuracy_score(y_test_sw, y_pred_sw))\n",
        "\n",
        "    # Training English\n",
        "    classifier_en = RandomForestClassifier(n_estimators = no_tree[i], criterion = 'entropy', random_state = 42)\n",
        "    classifier_en.fit(X_train_en_svm, y_train_en)\n",
        "    y_pred_en = classifier_en.predict(X_test_en_svm)\n",
        "    print(\"Accuracy:\",metrics.accuracy_score(y_test_en, y_pred_en))\n",
        "\n",
        "    # Training Gujarati\n",
        "    classifier_gu = RandomForestClassifier(n_estimators = no_tree[i], criterion = 'entropy', random_state = 42)\n",
        "    classifier_gu.fit(X_train_gu_svm, y_train_gu)\n",
        "    y_pred_gu = classifier_gu.predict(X_test_gu_svm)\n",
        "    print(\"Accuracy:\",metrics.accuracy_score(y_test_gu, y_pred_gu))\n",
        "    print(\"----------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no tree values: 10\n",
            "Accuracy: 0.7835051546391752\n",
            "Accuracy: 0.8762886597938144\n",
            "Accuracy: 0.6752577319587629\n",
            "----------------------------------------\n",
            "no tree values: 20\n",
            "Accuracy: 0.8015463917525774\n",
            "Accuracy: 0.8994845360824743\n",
            "Accuracy: 0.7731958762886598\n",
            "----------------------------------------\n",
            "no tree values: 30\n",
            "Accuracy: 0.8221649484536082\n",
            "Accuracy: 0.9097938144329897\n",
            "Accuracy: 0.7912371134020618\n",
            "----------------------------------------\n",
            "no tree values: 40\n",
            "Accuracy: 0.8247422680412371\n",
            "Accuracy: 0.9226804123711341\n",
            "Accuracy: 0.788659793814433\n",
            "----------------------------------------\n",
            "no tree values: 50\n",
            "Accuracy: 0.8221649484536082\n",
            "Accuracy: 0.9355670103092784\n",
            "Accuracy: 0.788659793814433\n",
            "----------------------------------------\n",
            "no tree values: 60\n",
            "Accuracy: 0.8402061855670103\n",
            "Accuracy: 0.9381443298969072\n",
            "Accuracy: 0.8015463917525774\n",
            "----------------------------------------\n",
            "no tree values: 70\n",
            "Accuracy: 0.8479381443298969\n",
            "Accuracy: 0.9407216494845361\n",
            "Accuracy: 0.7912371134020618\n",
            "----------------------------------------\n",
            "no tree values: 80\n",
            "Accuracy: 0.8479381443298969\n",
            "Accuracy: 0.9407216494845361\n",
            "Accuracy: 0.7989690721649485\n",
            "----------------------------------------\n",
            "no tree values: 90\n",
            "Accuracy: 0.8556701030927835\n",
            "Accuracy: 0.9355670103092784\n",
            "Accuracy: 0.8092783505154639\n",
            "----------------------------------------\n",
            "no tree values: 100\n",
            "Accuracy: 0.8556701030927835\n",
            "Accuracy: 0.9355670103092784\n",
            "Accuracy: 0.8041237113402062\n",
            "----------------------------------------\n",
            "no tree values: 110\n",
            "Accuracy: 0.8505154639175257\n",
            "Accuracy: 0.9355670103092784\n",
            "Accuracy: 0.8144329896907216\n",
            "----------------------------------------\n",
            "no tree values: 120\n",
            "Accuracy: 0.8505154639175257\n",
            "Accuracy: 0.9381443298969072\n",
            "Accuracy: 0.8195876288659794\n",
            "----------------------------------------\n",
            "no tree values: 130\n",
            "Accuracy: 0.8556701030927835\n",
            "Accuracy: 0.9432989690721649\n",
            "Accuracy: 0.8170103092783505\n",
            "----------------------------------------\n",
            "no tree values: 140\n",
            "Accuracy: 0.8556701030927835\n",
            "Accuracy: 0.9407216494845361\n",
            "Accuracy: 0.8247422680412371\n",
            "----------------------------------------\n",
            "no tree values: 150\n",
            "Accuracy: 0.8634020618556701\n",
            "Accuracy: 0.9407216494845361\n",
            "Accuracy: 0.8195876288659794\n",
            "----------------------------------------\n",
            "no tree values: 160\n",
            "Accuracy: 0.865979381443299\n",
            "Accuracy: 0.9381443298969072\n",
            "Accuracy: 0.8298969072164949\n",
            "----------------------------------------\n",
            "no tree values: 170\n",
            "Accuracy: 0.8634020618556701\n",
            "Accuracy: 0.9355670103092784\n",
            "Accuracy: 0.8324742268041238\n",
            "----------------------------------------\n",
            "no tree values: 180\n",
            "Accuracy: 0.8582474226804123\n",
            "Accuracy: 0.9329896907216495\n",
            "Accuracy: 0.8221649484536082\n",
            "----------------------------------------\n",
            "no tree values: 190\n",
            "Accuracy: 0.8582474226804123\n",
            "Accuracy: 0.9329896907216495\n",
            "Accuracy: 0.8170103092783505\n",
            "----------------------------------------\n",
            "no tree values: 200\n",
            "Accuracy: 0.865979381443299\n",
            "Accuracy: 0.9329896907216495\n",
            "Accuracy: 0.8144329896907216\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFgsTdrEKNg8",
        "outputId": "2f8ed693-4b5f-4a35-c0e2-49c7eefe62cd"
      },
      "source": [
        "# Training and Evaluating knn\n",
        "\n",
        "for k in range(20):\n",
        "    print(\"k neighbors value:\",k+1) \n",
        "    # Training Swahili\n",
        "    knn_sw = KNeighborsClassifier(n_neighbors=k+1)\n",
        "    knn_sw.fit(X_train_sw_svm, y_train_sw)\n",
        "    y_pred_sw = knn_sw.predict(X_test_sw_svm)\n",
        "    print(\"Accuracy:\",metrics.accuracy_score(y_test_sw, y_pred_sw))\n",
        "\n",
        "    # Training English\n",
        "    knn_en = KNeighborsClassifier(n_neighbors=k+1)\n",
        "    knn_en.fit(X_train_en_svm, y_train_en)\n",
        "    y_pred_en = knn_en.predict(X_test_en_svm)\n",
        "    print(\"Accuracy:\",metrics.accuracy_score(y_test_en, y_pred_en))\n",
        "\n",
        "    # Training Swahili\n",
        "    knn_gu = KNeighborsClassifier(n_neighbors=k+1)\n",
        "    knn_gu.fit(X_train_gu_svm, y_train_gu)\n",
        "    y_pred_gu = knn_gu.predict(X_test_gu_svm)\n",
        "    print(\"Accuracy:\",metrics.accuracy_score(y_test_gu, y_pred_gu))\n",
        "    print (\"------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k neighbors value: 1\n",
            "Accuracy: 0.7319587628865979\n",
            "Accuracy: 0.904639175257732\n",
            "Accuracy: 0.7680412371134021\n",
            "------------------------------------------\n",
            "k neighbors value: 2\n",
            "Accuracy: 0.6907216494845361\n",
            "Accuracy: 0.8994845360824743\n",
            "Accuracy: 0.7216494845360825\n",
            "------------------------------------------\n",
            "k neighbors value: 3\n",
            "Accuracy: 0.6881443298969072\n",
            "Accuracy: 0.8917525773195877\n",
            "Accuracy: 0.7371134020618557\n",
            "------------------------------------------\n",
            "k neighbors value: 4\n",
            "Accuracy: 0.6881443298969072\n",
            "Accuracy: 0.884020618556701\n",
            "Accuracy: 0.7396907216494846\n",
            "------------------------------------------\n",
            "k neighbors value: 5\n",
            "Accuracy: 0.6829896907216495\n",
            "Accuracy: 0.8634020618556701\n",
            "Accuracy: 0.7242268041237113\n",
            "------------------------------------------\n",
            "k neighbors value: 6\n",
            "Accuracy: 0.6701030927835051\n",
            "Accuracy: 0.8711340206185567\n",
            "Accuracy: 0.7036082474226805\n",
            "------------------------------------------\n",
            "k neighbors value: 7\n",
            "Accuracy: 0.6752577319587629\n",
            "Accuracy: 0.8685567010309279\n",
            "Accuracy: 0.6984536082474226\n",
            "------------------------------------------\n",
            "k neighbors value: 8\n",
            "Accuracy: 0.6778350515463918\n",
            "Accuracy: 0.865979381443299\n",
            "Accuracy: 0.6881443298969072\n",
            "------------------------------------------\n",
            "k neighbors value: 9\n",
            "Accuracy: 0.6752577319587629\n",
            "Accuracy: 0.8634020618556701\n",
            "Accuracy: 0.6907216494845361\n",
            "------------------------------------------\n",
            "k neighbors value: 10\n",
            "Accuracy: 0.6675257731958762\n",
            "Accuracy: 0.8737113402061856\n",
            "Accuracy: 0.6675257731958762\n",
            "------------------------------------------\n",
            "k neighbors value: 11\n",
            "Accuracy: 0.6623711340206185\n",
            "Accuracy: 0.8634020618556701\n",
            "Accuracy: 0.6675257731958762\n",
            "------------------------------------------\n",
            "k neighbors value: 12\n",
            "Accuracy: 0.6494845360824743\n",
            "Accuracy: 0.8479381443298969\n",
            "Accuracy: 0.6778350515463918\n",
            "------------------------------------------\n",
            "k neighbors value: 13\n",
            "Accuracy: 0.6469072164948454\n",
            "Accuracy: 0.8376288659793815\n",
            "Accuracy: 0.6726804123711341\n",
            "------------------------------------------\n",
            "k neighbors value: 14\n",
            "Accuracy: 0.634020618556701\n",
            "Accuracy: 0.8221649484536082\n",
            "Accuracy: 0.6855670103092784\n",
            "------------------------------------------\n",
            "k neighbors value: 15\n",
            "Accuracy: 0.6365979381443299\n",
            "Accuracy: 0.8221649484536082\n",
            "Accuracy: 0.6701030927835051\n",
            "------------------------------------------\n",
            "k neighbors value: 16\n",
            "Accuracy: 0.6417525773195877\n",
            "Accuracy: 0.8144329896907216\n",
            "Accuracy: 0.6649484536082474\n",
            "------------------------------------------\n",
            "k neighbors value: 17\n",
            "Accuracy: 0.6288659793814433\n",
            "Accuracy: 0.8195876288659794\n",
            "Accuracy: 0.6623711340206185\n",
            "------------------------------------------\n",
            "k neighbors value: 18\n",
            "Accuracy: 0.634020618556701\n",
            "Accuracy: 0.8170103092783505\n",
            "Accuracy: 0.6572164948453608\n",
            "------------------------------------------\n",
            "k neighbors value: 19\n",
            "Accuracy: 0.6443298969072165\n",
            "Accuracy: 0.8170103092783505\n",
            "Accuracy: 0.6443298969072165\n",
            "------------------------------------------\n",
            "k neighbors value: 20\n",
            "Accuracy: 0.6417525773195877\n",
            "Accuracy: 0.8041237113402062\n",
            "Accuracy: 0.6494845360824743\n",
            "------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5V-jwxRKNg8"
      },
      "source": [
        "# Table 5 Multi-lingual data \n",
        "# Note: Run this script only for Multi-lingual model training\n",
        "\n",
        "# English + Gujarati combination datasets\n",
        "X_train_en_gu=np.concatenate((X_train_en,X_train_gu))\n",
        "y_train_en_gu=np.concatenate((y_train_en,y_train_gu))\n",
        "\n",
        "# Gujarati + Swahili combination datasets\n",
        "X_train_gu_sw=np.concatenate((X_train_gu,X_train_sw))\n",
        "y_train_gu_sw=np.concatenate((y_train_gu,y_train_sw))\n",
        "\n",
        "# Swahili + English combination datasets\n",
        "X_train_sw_en=np.concatenate((X_train_sw,X_train_en))\n",
        "y_train_sw_en=np.concatenate((y_train_sw,y_train_en))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88RNYaJaKNg9",
        "outputId": "eb653651-fd41-4bec-b67e-12daf4d7be49"
      },
      "source": [
        "len(y_train_sw_en)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3104"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtjrwBRsKNg9",
        "outputId": "10f0ef7b-6700-4cb5-915b-c4541cae787c"
      },
      "source": [
        "y_train_gu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['3', '0', '3', ..., '9', '1', '5'], dtype='<U1')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "QbQUssHtKNg_",
        "outputId": "797f6bd7-9a22-40e9-caee-4c4347adf886"
      },
      "source": [
        "# Pre- Training model\n",
        "# Parameters\n",
        "epoch_l = [1,5,10,20,30]  # no of epochs\n",
        "lan=['sw','en','gu']      #languages\n",
        "lan_str=[\"Swahili\",\"English\",\"Gujarati\"]   #languages string\n",
        "\n",
        "# Monolingual or Crosslingual case\n",
        "source=[X_train_sw,X_train_en,X_train_gu]  #source dataset features\n",
        "target=[y_train_sw,y_train_en,y_train_gu]  #target dataset features\n",
        "\n",
        "# Multi-lingual case\n",
        "source_comb=[X_train_en_gu,X_train_gu_sw,X_train_sw_en]  #source dataset features\n",
        "target_comb=[y_train_en_gu,y_train_gu_sw,y_train_sw_en]  #target dataset features\n",
        "\n",
        "# Looping each source language and across no of epochs\n",
        "for j in range(len(lan)):\n",
        "    print(\"Source language: \"+lan_str[j])\n",
        "    for i in range(len(epoch_l)):\n",
        "        reset_random_seeds()\n",
        "\n",
        "        #change data type\n",
        "        X_train_en=X_train_en.astype(\"float32\")\n",
        "        X_test_en=X_test_en.astype(\"float32\")\n",
        "\n",
        "        X_train_sw=X_train_sw.astype(\"float32\")\n",
        "        X_test_sw=X_test_sw.astype(\"float32\")\n",
        "\n",
        "        X_train_gu=X_train_gu.astype(\"float32\")\n",
        "        X_test_gu=X_test_gu.astype(\"float32\")\n",
        "\n",
        "\n",
        "        #define history container\n",
        "        history=[]\n",
        "\n",
        "        #for train, test in kfold.split(inputs, targets):\n",
        "        ## Define Auto-encoder \n",
        "        latent_space_dim=64\n",
        "        # Table 3\n",
        "        ## Input shape definition\n",
        "        input_shape=keras.layers.Input(shape=(X_train_en.shape[1], X_train_en.shape[2], X_train_en.shape[3]))\n",
        "\n",
        "        ## Encode layers\n",
        "        #1st conv layer\n",
        "        conv1=keras.layers.Conv2D(32,3,(1,1),padding='same')(input_shape)\n",
        "        conv1=keras.layers.ReLU()(conv1)\n",
        "        conv1=keras.layers.BatchNormalization()(conv1)\n",
        "\n",
        "        #2nd conv layer\n",
        "        conv2=keras.layers.Conv2D(64,3,(2,2),padding='same')(conv1)\n",
        "        conv2=keras.layers.ReLU()(conv2)\n",
        "        conv2=keras.layers.BatchNormalization()(conv2)\n",
        "\n",
        "        #3rd conv layer\n",
        "        conv3=keras.layers.Conv2D(64,3,(2,2),padding='same')(conv2)\n",
        "        conv3=keras.layers.ReLU()(conv3)\n",
        "        conv3=keras.layers.BatchNormalization()(conv3)\n",
        "\n",
        "        #shape before flatten\n",
        "        shape_before_flatten = keras.backend.int_shape(conv3)[1:]  #conv3  ,conv2\n",
        "\n",
        "        #flatten the output and feed it into dense layer (bottleneck)\n",
        "        conv_flatten=keras.layers.Flatten()(conv3)   #conv3     ,conv2\n",
        "        encoded=keras.layers.Dense(latent_space_dim,activation='relu')(conv_flatten)\n",
        "\n",
        "        ## baseline model classifier \n",
        "        drop=keras.layers.Dropout(0.3,seed=1)(encoded)\n",
        "        fc=keras.layers.Dense(10,activation='softmax')(drop)\n",
        "        \n",
        "        # Model\n",
        "        full_model = keras.Model(input_shape,fc)\n",
        "\n",
        "        #compile the network\n",
        "        optimizer=keras.optimizers.Adam(lr=0.0001)\n",
        "        full_model.compile(optimizer=optimizer,\n",
        "                     loss=\"sparse_categorical_crossentropy\",\n",
        "                     metrics=['acc'])\n",
        "\n",
        "        ## monolingual Pre-training\n",
        "        filename='test-Single-L3/CNN-digit-mfcc-T3-E'+str(epoch_l[i])+'-S'+lan[j]+'.csv'\n",
        "        ##track log\n",
        "        history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
        "        ##Training\n",
        "        #full_model.save(\"Single-L3/CNN-digit-mfcc-T3-Single\")\n",
        "        print(\"For Epoch: \"+str(epoch_l[i]))\n",
        "        history=full_model.fit(source[j],target[j],batch_size=32,epochs=epoch_l[i],verbose=1,callbacks=[history_logger]) #,callbacks=[history_logger]\n",
        "        \n",
        "        ##Save the model\n",
        "        full_model.save(\"test-Single-L3/CNN-digit-mfcc-T3-E\"+str(epoch_l[i])+\"-S\"+lan[j])\n",
        "        #full_model.save(\"Single-L3/CNN-digit-mfcc-T3-Single\")\n",
        "        #reset\n",
        "        tf.reset_default_graph()\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        ## multi-lingual Pre-training\n",
        "#        filename='Single-L3-two/reverse/CNN-digit-mfcc-T3-E'+str(epoch_l[i])+'-S'+lan[j]+'.csv'\n",
        "        ##track log\n",
        "#        history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
        "        ##Training\n",
        "        #full_model.save(\"Single-L3/CNN-digit-mfcc-T3-Single\")\n",
        "#        print(\"For Epoch: \"+str(epoch_l[i]))\n",
        "#        history=full_model.fit(source_comb[j],target_comb[j],batch_size=32,epochs=epoch_l[i],verbose=1,callbacks=[history_logger]) #,callbacks=[history_logger]\n",
        "        \n",
        "        ##Save the model\n",
        "#        full_model.save(\"Single-L3-two/reverse/CNN-digit-mfcc-T3-E\"+str(epoch_l[i])+\"-S\"+lan[j])\n",
        "        #full_model.save(\"Single-L3/CNN-digit-mfcc-T3-Single\")\n",
        "        #reset\n",
        "#        tf.reset_default_graph()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source language: Swahili\n",
            "For Epoch: 1\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 2.1634 - acc: 0.2371\n",
            "For Epoch: 5\n",
            "Epoch 1/5\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 2.1634 - acc: 0.2371\n",
            "Epoch 2/5\n",
            "1552/1552 [==============================] - 25s 16ms/sample - loss: 1.6092 - acc: 0.4066\n",
            "Epoch 3/5\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 1.2806 - acc: 0.5329\n",
            "Epoch 4/5\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 1.0679 - acc: 0.5992\n",
            "Epoch 5/5\n",
            "1552/1552 [==============================] - 26s 17ms/sample - loss: 0.9280 - acc: 0.6714\n",
            "For Epoch: 10\n",
            "Epoch 1/10\n",
            "1552/1552 [==============================] - 25s 16ms/sample - loss: 2.1634 - acc: 0.2371\n",
            "Epoch 2/10\n",
            "1552/1552 [==============================] - 27s 17ms/sample - loss: 1.6092 - acc: 0.4066\n",
            "Epoch 3/10\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 1.2806 - acc: 0.5329\n",
            "Epoch 4/10\n",
            "1552/1552 [==============================] - 27s 17ms/sample - loss: 1.0679 - acc: 0.5992\n",
            "Epoch 5/10\n",
            "1552/1552 [==============================] - 24s 16ms/sample - loss: 0.9280 - acc: 0.6714\n",
            "Epoch 6/10\n",
            "1552/1552 [==============================] - 25s 16ms/sample - loss: 0.7992 - acc: 0.7120\n",
            "Epoch 7/10\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.6635 - acc: 0.7803\n",
            "Epoch 8/10\n",
            "1552/1552 [==============================] - 25s 16ms/sample - loss: 0.5942 - acc: 0.7841\n",
            "Epoch 9/10\n",
            "1552/1552 [==============================] - 25s 16ms/sample - loss: 0.5385 - acc: 0.8164\n",
            "Epoch 10/10\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.4884 - acc: 0.8351\n",
            "For Epoch: 20\n",
            "Epoch 1/20\n",
            "1552/1552 [==============================] - 26s 17ms/sample - loss: 2.1634 - acc: 0.2371\n",
            "Epoch 2/20\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 1.6092 - acc: 0.4066\n",
            "Epoch 3/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 1.2806 - acc: 0.5329\n",
            "Epoch 4/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 1.0679 - acc: 0.5992\n",
            "Epoch 5/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.9280 - acc: 0.6714\n",
            "Epoch 6/20\n",
            "1552/1552 [==============================] - 27s 17ms/sample - loss: 0.7992 - acc: 0.7120\n",
            "Epoch 7/20\n",
            "1552/1552 [==============================] - 24s 16ms/sample - loss: 0.6635 - acc: 0.7803\n",
            "Epoch 8/20\n",
            "1552/1552 [==============================] - 24s 16ms/sample - loss: 0.5942 - acc: 0.7841\n",
            "Epoch 9/20\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.5385 - acc: 0.8164\n",
            "Epoch 10/20\n",
            "1552/1552 [==============================] - 25s 16ms/sample - loss: 0.4884 - acc: 0.8351\n",
            "Epoch 11/20\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.4332 - acc: 0.8589\n",
            "Epoch 12/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.3656 - acc: 0.8827\n",
            "Epoch 13/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.3488 - acc: 0.8885\n",
            "Epoch 14/20\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.3053 - acc: 0.9091\n",
            "Epoch 15/20\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.3001 - acc: 0.9130\n",
            "Epoch 16/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.2477 - acc: 0.9323\n",
            "Epoch 17/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.2367 - acc: 0.9343\n",
            "Epoch 18/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.2212 - acc: 0.9375\n",
            "Epoch 19/20\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.1989 - acc: 0.9497\n",
            "Epoch 20/20\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.1717 - acc: 0.9517\n",
            "For Epoch: 30\n",
            "Epoch 1/30\n",
            "1552/1552 [==============================] - 24s 16ms/sample - loss: 2.1634 - acc: 0.2371\n",
            "Epoch 2/30\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 1.6092 - acc: 0.4066\n",
            "Epoch 3/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 1.2806 - acc: 0.5329\n",
            "Epoch 4/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 1.0679 - acc: 0.5992\n",
            "Epoch 5/30\n",
            "1552/1552 [==============================] - 25s 16ms/sample - loss: 0.9280 - acc: 0.6714\n",
            "Epoch 6/30\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.7992 - acc: 0.7120\n",
            "Epoch 7/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.6635 - acc: 0.7803\n",
            "Epoch 8/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.5942 - acc: 0.7841\n",
            "Epoch 9/30\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.5385 - acc: 0.8164\n",
            "Epoch 10/30\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.4884 - acc: 0.8351\n",
            "Epoch 11/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.4332 - acc: 0.8589\n",
            "Epoch 12/30\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.3656 - acc: 0.8827\n",
            "Epoch 13/30\n",
            "1552/1552 [==============================] - 25s 16ms/sample - loss: 0.3488 - acc: 0.8885\n",
            "Epoch 14/30\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.3053 - acc: 0.9091\n",
            "Epoch 15/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.3001 - acc: 0.9130\n",
            "Epoch 16/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.2477 - acc: 0.9323\n",
            "Epoch 17/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.2367 - acc: 0.9343\n",
            "Epoch 18/30\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.2212 - acc: 0.9375\n",
            "Epoch 19/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1989 - acc: 0.9497\n",
            "Epoch 20/30\n",
            "1552/1552 [==============================] - 26s 17ms/sample - loss: 0.1717 - acc: 0.9517\n",
            "Epoch 21/30\n",
            "1552/1552 [==============================] - 24s 16ms/sample - loss: 0.1682 - acc: 0.9575\n",
            "Epoch 22/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1328 - acc: 0.9671\n",
            "Epoch 23/30\n",
            "1552/1552 [==============================] - 25s 16ms/sample - loss: 0.1384 - acc: 0.9652\n",
            "Epoch 24/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1269 - acc: 0.9678\n",
            "Epoch 25/30\n",
            "1552/1552 [==============================] - 26s 17ms/sample - loss: 0.1177 - acc: 0.9646\n",
            "Epoch 26/30\n",
            "1552/1552 [==============================] - 26s 17ms/sample - loss: 0.1160 - acc: 0.9774\n",
            "Epoch 27/30\n",
            "1552/1552 [==============================] - 28s 18ms/sample - loss: 0.1117 - acc: 0.9736\n",
            "Epoch 28/30\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.1088 - acc: 0.9749\n",
            "Epoch 29/30\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.0944 - acc: 0.9736\n",
            "Epoch 30/30\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.0958 - acc: 0.9762\n",
            "Source language: English\n",
            "For Epoch: 1\n",
            "1552/1552 [==============================] - 24s 16ms/sample - loss: 2.1270 - acc: 0.2345\n",
            "For Epoch: 5\n",
            "Epoch 1/5\n",
            "1552/1552 [==============================] - 25s 16ms/sample - loss: 2.1270 - acc: 0.2345\n",
            "Epoch 2/5\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 1.6093 - acc: 0.4536\n",
            "Epoch 3/5\n",
            "1552/1552 [==============================] - 29s 19ms/sample - loss: 1.2866 - acc: 0.5728\n",
            "Epoch 4/5\n",
            "1552/1552 [==============================] - 24s 16ms/sample - loss: 1.0043 - acc: 0.6914\n",
            "Epoch 5/5\n",
            "1552/1552 [==============================] - 24s 16ms/sample - loss: 0.7783 - acc: 0.7668\n",
            "For Epoch: 10\n",
            "Epoch 1/10\n",
            "1552/1552 [==============================] - 24s 16ms/sample - loss: 2.1270 - acc: 0.2345\n",
            "Epoch 2/10\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 1.6093 - acc: 0.4536\n",
            "Epoch 3/10\n",
            "1552/1552 [==============================] - 24s 16ms/sample - loss: 1.2866 - acc: 0.5728\n",
            "Epoch 4/10\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 1.0043 - acc: 0.6914\n",
            "Epoch 5/10\n",
            "1552/1552 [==============================] - 27s 17ms/sample - loss: 0.7783 - acc: 0.7668\n",
            "Epoch 6/10\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.6434 - acc: 0.8144\n",
            "Epoch 7/10\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.5457 - acc: 0.8396\n",
            "Epoch 8/10\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.4283 - acc: 0.8892\n",
            "Epoch 9/10\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.3774 - acc: 0.8930\n",
            "Epoch 10/10\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.3371 - acc: 0.9021\n",
            "For Epoch: 20\n",
            "Epoch 1/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 2.1270 - acc: 0.2345\n",
            "Epoch 2/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 1.6093 - acc: 0.4536\n",
            "Epoch 3/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 1.2866 - acc: 0.5728\n",
            "Epoch 4/20\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 1.0043 - acc: 0.6914\n",
            "Epoch 5/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.7783 - acc: 0.7668\n",
            "Epoch 6/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.6434 - acc: 0.8144\n",
            "Epoch 7/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.5457 - acc: 0.8396\n",
            "Epoch 8/20\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.4283 - acc: 0.8892\n",
            "Epoch 9/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.3774 - acc: 0.8930\n",
            "Epoch 10/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.3371 - acc: 0.9021\n",
            "Epoch 11/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.2911 - acc: 0.9285\n",
            "Epoch 12/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.2554 - acc: 0.9317\n",
            "Epoch 13/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.2467 - acc: 0.9369\n",
            "Epoch 14/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1977 - acc: 0.9530\n",
            "Epoch 15/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1693 - acc: 0.9536\n",
            "Epoch 16/20\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.1765 - acc: 0.9536\n",
            "Epoch 17/20\n",
            "1552/1552 [==============================] - 24s 16ms/sample - loss: 0.1584 - acc: 0.9568\n",
            "Epoch 18/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1430 - acc: 0.9613\n",
            "Epoch 19/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1348 - acc: 0.9691\n",
            "Epoch 20/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1101 - acc: 0.9768\n",
            "For Epoch: 30\n",
            "Epoch 1/30\n",
            "1552/1552 [==============================] - 25s 16ms/sample - loss: 2.1270 - acc: 0.2345\n",
            "Epoch 2/30\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 1.6093 - acc: 0.4536\n",
            "Epoch 3/30\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 1.2866 - acc: 0.5728\n",
            "Epoch 4/30\n",
            "1552/1552 [==============================] - 26s 17ms/sample - loss: 1.0043 - acc: 0.6914\n",
            "Epoch 5/30\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.7783 - acc: 0.7668\n",
            "Epoch 6/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.6434 - acc: 0.8144\n",
            "Epoch 7/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.5457 - acc: 0.8396\n",
            "Epoch 8/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.4283 - acc: 0.8892\n",
            "Epoch 9/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.3774 - acc: 0.8930\n",
            "Epoch 10/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.3371 - acc: 0.9021\n",
            "Epoch 11/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.2911 - acc: 0.9285\n",
            "Epoch 12/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.2554 - acc: 0.9317\n",
            "Epoch 13/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.2467 - acc: 0.9369\n",
            "Epoch 14/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1977 - acc: 0.9530\n",
            "Epoch 15/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1693 - acc: 0.9536\n",
            "Epoch 16/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1765 - acc: 0.9536\n",
            "Epoch 17/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1584 - acc: 0.9568\n",
            "Epoch 18/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1430 - acc: 0.9613\n",
            "Epoch 19/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1348 - acc: 0.9691\n",
            "Epoch 20/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1101 - acc: 0.9768\n",
            "Epoch 21/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1030 - acc: 0.9749\n",
            "Epoch 22/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1064 - acc: 0.9723\n",
            "Epoch 23/30\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.0879 - acc: 0.9794\n",
            "Epoch 24/30\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.0836 - acc: 0.9826\n",
            "Epoch 25/30\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.0782 - acc: 0.9813\n",
            "Epoch 26/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.0774 - acc: 0.9826\n",
            "Epoch 27/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.0689 - acc: 0.9820\n",
            "Epoch 28/30\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.0798 - acc: 0.9813\n",
            "Epoch 29/30\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.0698 - acc: 0.9813\n",
            "Epoch 30/30\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.0518 - acc: 0.9884\n",
            "Source language: Gujarati\n",
            "For Epoch: 1\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 2.2140 - acc: 0.2088\n",
            "For Epoch: 5\n",
            "Epoch 1/5\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 2.2140 - acc: 0.2088\n",
            "Epoch 2/5\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 1.7090 - acc: 0.4008\n",
            "Epoch 3/5\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 1.3612 - acc: 0.5116\n",
            "Epoch 4/5\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 1.0968 - acc: 0.6250\n",
            "Epoch 5/5\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.9166 - acc: 0.6933\n",
            "For Epoch: 10\n",
            "Epoch 1/10\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 2.2140 - acc: 0.2088\n",
            "Epoch 2/10\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 1.7090 - acc: 0.4008\n",
            "Epoch 3/10\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 1.3612 - acc: 0.5116\n",
            "Epoch 4/10\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 1.0968 - acc: 0.6250\n",
            "Epoch 5/10\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.9166 - acc: 0.6933\n",
            "Epoch 6/10\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.7568 - acc: 0.7455\n",
            "Epoch 7/10\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.6655 - acc: 0.7809\n",
            "Epoch 8/10\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.6047 - acc: 0.7970\n",
            "Epoch 9/10\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.5108 - acc: 0.8421\n",
            "Epoch 10/10\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.4670 - acc: 0.8512\n",
            "For Epoch: 20\n",
            "Epoch 1/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 2.2140 - acc: 0.2088\n",
            "Epoch 2/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 1.7090 - acc: 0.4008\n",
            "Epoch 3/20\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 1.3612 - acc: 0.5116\n",
            "Epoch 4/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 1.0968 - acc: 0.6250\n",
            "Epoch 5/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.9166 - acc: 0.6933\n",
            "Epoch 6/20\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.7568 - acc: 0.7455\n",
            "Epoch 7/20\n",
            "1552/1552 [==============================] - 26s 17ms/sample - loss: 0.6655 - acc: 0.7809\n",
            "Epoch 8/20\n",
            "1552/1552 [==============================] - 1549s 998ms/sample - loss: 0.6047 - acc: 0.7970\n",
            "Epoch 9/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.5108 - acc: 0.8421\n",
            "Epoch 10/20\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.4670 - acc: 0.8512\n",
            "Epoch 11/20\n",
            "1552/1552 [==============================] - 23s 14ms/sample - loss: 0.4350 - acc: 0.8634\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/20\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.3667 - acc: 0.8834\n",
            "Epoch 13/20\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.3668 - acc: 0.8853\n",
            "Epoch 14/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.2741 - acc: 0.9182\n",
            "Epoch 15/20\n",
            "1552/1552 [==============================] - 25s 16ms/sample - loss: 0.2752 - acc: 0.9124\n",
            "Epoch 16/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.2483 - acc: 0.9311\n",
            "Epoch 17/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.2120 - acc: 0.9394\n",
            "Epoch 18/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.2174 - acc: 0.9336\n",
            "Epoch 19/20\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.1989 - acc: 0.9394\n",
            "Epoch 20/20\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1621 - acc: 0.9562\n",
            "For Epoch: 30\n",
            "Epoch 1/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 2.2140 - acc: 0.2088\n",
            "Epoch 2/30\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 1.7090 - acc: 0.4008\n",
            "Epoch 3/30\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 1.3612 - acc: 0.5116\n",
            "Epoch 4/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 1.0968 - acc: 0.6250\n",
            "Epoch 5/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.9166 - acc: 0.6933\n",
            "Epoch 6/30\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.7568 - acc: 0.7455\n",
            "Epoch 7/30\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.6655 - acc: 0.7809\n",
            "Epoch 8/30\n",
            "1552/1552 [==============================] - 27s 17ms/sample - loss: 0.6047 - acc: 0.7970\n",
            "Epoch 9/30\n",
            "1552/1552 [==============================] - 32s 20ms/sample - loss: 0.5108 - acc: 0.8421\n",
            "Epoch 10/30\n",
            "1552/1552 [==============================] - 29s 19ms/sample - loss: 0.4670 - acc: 0.8512\n",
            "Epoch 11/30\n",
            "1552/1552 [==============================] - 30s 19ms/sample - loss: 0.4350 - acc: 0.8634\n",
            "Epoch 12/30\n",
            "1552/1552 [==============================] - 31s 20ms/sample - loss: 0.3667 - acc: 0.8834\n",
            "Epoch 13/30\n",
            "1552/1552 [==============================] - 30s 19ms/sample - loss: 0.3668 - acc: 0.8853\n",
            "Epoch 14/30\n",
            "1552/1552 [==============================] - 30s 19ms/sample - loss: 0.2741 - acc: 0.9182\n",
            "Epoch 15/30\n",
            "1552/1552 [==============================] - 30s 19ms/sample - loss: 0.2752 - acc: 0.9124\n",
            "Epoch 16/30\n",
            "1552/1552 [==============================] - 30s 19ms/sample - loss: 0.2483 - acc: 0.9311\n",
            "Epoch 17/30\n",
            "1552/1552 [==============================] - 30s 19ms/sample - loss: 0.2120 - acc: 0.9394\n",
            "Epoch 18/30\n",
            "1552/1552 [==============================] - 30s 20ms/sample - loss: 0.2174 - acc: 0.9336\n",
            "Epoch 19/30\n",
            "1552/1552 [==============================] - 31s 20ms/sample - loss: 0.1989 - acc: 0.9394\n",
            "Epoch 20/30\n",
            "1552/1552 [==============================] - 30s 19ms/sample - loss: 0.1621 - acc: 0.9562\n",
            "Epoch 21/30\n",
            "1552/1552 [==============================] - 25s 16ms/sample - loss: 0.1462 - acc: 0.9646\n",
            "Epoch 22/30\n",
            "1552/1552 [==============================] - 24s 15ms/sample - loss: 0.1316 - acc: 0.9646\n",
            "Epoch 23/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1262 - acc: 0.9646\n",
            "Epoch 24/30\n",
            "1552/1552 [==============================] - 25s 16ms/sample - loss: 0.1204 - acc: 0.9697\n",
            "Epoch 25/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1084 - acc: 0.9768\n",
            "Epoch 26/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1043 - acc: 0.9768\n",
            "Epoch 27/30\n",
            "1552/1552 [==============================] - 23s 15ms/sample - loss: 0.1022 - acc: 0.9781\n",
            "Epoch 28/30\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.0920 - acc: 0.9774\n",
            "Epoch 29/30\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.0816 - acc: 0.9845\n",
            "Epoch 30/30\n",
            "1552/1552 [==============================] - 22s 14ms/sample - loss: 0.0778 - acc: 0.9832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sdWrlFhKNhA"
      },
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbb62VZaKNhB",
        "outputId": "c9d9c7f7-d584-4df3-f5ca-2652c4976923"
      },
      "source": [
        "## Monolingual classic, Monolingual+Pre and Cross-lingual target training and evaluation\n",
        "epoch_l= [1,5,10,20,30]\n",
        "#ta=['gu']\n",
        "lan=['sw','en','gu']   #['sw','en','gu']\n",
        "lan_str=[\"Swahili\",\"English\",\"Gujarati\"]   #[\"Swahili\",\"English\",\"Gujarati\"]\n",
        "source_tr=[X_train_sw,X_train_en,X_train_gu]  #[X_train_sw,X_train_en,X_train_gu]\n",
        "target_tr=[y_train_sw,y_train_en,y_train_gu]   #[y_train_sw,y_train_en,y_train_gu]\n",
        "source_te=[X_test_sw,X_test_en,X_test_gu]  #[X_train_sw,X_train_en,X_train_gu]\n",
        "target_te=[y_test_sw,y_test_en,y_test_gu]   #[y_train_sw,y_train_en,y_train_gu]\n",
        "for i in range(len(lan)):\n",
        "    print(\"Target language: \"+lan_str[i])\n",
        "    for j in range(len(lan)):\n",
        "        print(\"Source language: \"+lan_str[j])\n",
        "        for k in range(len(epoch_l)):\n",
        "            print(\"For Epoch :\"+str(epoch_l[k]))\n",
        "            reset_random_seeds()\n",
        "            # 1. Direct results\n",
        " #           filename='Results_Single-L3-cross-lingual/CNN-digit-mfcc-T3-E'+str(epoch_l[k])+'-S'+lan[j]+'-D'+lan[i]+'.csv'\n",
        "            ##track log\n",
        "#            history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
        "            # Load direct models\n",
        "            model_load_test = keras.models.load_model(\"Single-L3/CNN-digit-mfcc-T3-E\"+str(epoch_l[4])+\"-S\"+lan[j])\n",
        "            \n",
        "            # Evaluate the target languages for Monolingual classic\n",
        "            test_error,test_accuracy=model_load_test.evaluate(source_te[i],target_te[i],verbose=1)\n",
        "            print(\"Accuracy on test set is: {}\".format(test_accuracy))\n",
        "            \n",
        "            # Evaluate the target languages for Monolingual+Pre, Cross-lingual\n",
        "            history=model_load_test.fit(source_tr[i],target_tr[i],validation_data=(source_te[i],target_te[i]),batch_size=32,epochs=30,verbose=1)  #,,callbacks=[history_logger]\n",
        "            # 1. Direct saved model\n",
        "#           model_load_test.save('Results_Single-L3-cross-lingual/CNN-digit-mfcc-T3-E'+str(epoch_l[k])+'-S'+lan[j]+'-D'+lan[i])\n",
        "            \n",
        "#tf.reset_default_graph()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target language: Swahili\n",
            "Source language: Swahili\n",
            "For Epoch :1\n",
            "388/388 [==============================] - 1s 2ms/sample - loss: 0.2808 - acc: 0.9149\n",
            "Accuracy on test set is: 0.9149484634399414\n",
            "Train on 1552 samples, validate on 388 samples\n",
            "Epoch 1/30\n",
            "1552/1552 [==============================] - 26s 17ms/sample - loss: 0.3339 - acc: 0.8834 - val_loss: 0.5493 - val_acc: 0.7964\n",
            "Epoch 2/30\n",
            "  64/1552 [>.............................] - ETA: 24s - loss: 0.4243 - acc: 0.8281"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-18-c3a3bb83964d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;31m# Evaluate the target languages for Monolingual+Pre, Cross-lingual\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_load_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_te\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_te\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#,,callbacks=[history_logger]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[1;31m# 1. Direct saved model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m#           model_load_test.save('Results_Single-L3-cross-lingual/CNN-digit-mfcc-T3-E'+str(epoch_l[k])+'-S'+lan[j]+'-D'+lan[i])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m   def evaluate(self,\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KLYdT25FKNhC",
        "outputId": "cefacde6-c823-414f-fd75-45e5845f2f2b"
      },
      "source": [
        "## Multi-lingual for direct and reverse target training and evaluation\n",
        "epoch_l= [1,5,10,20,30]\n",
        "#ta=['gu']\n",
        "lan=['sw','en','gu']   #['sw','en','gu']\n",
        "lan_str=[\"Swahili\",\"English\",\"Gujarati\"]   #[\"Swahili\",\"English\",\"Gujarati\"]\n",
        "source_tr=[X_train_sw,X_train_en,X_train_gu]  #[X_train_sw,X_train_en,X_train_gu]\n",
        "target_tr=[y_train_sw,y_train_en,y_train_gu]   #[y_train_sw,y_train_en,y_train_gu]\n",
        "source_te=[X_test_sw,X_test_en,X_test_gu]  #[X_train_sw,X_train_en,X_train_gu]\n",
        "target_te=[y_test_sw,y_test_en,y_test_gu]   #[y_train_sw,y_train_en,y_train_gu]\n",
        "for i in range(len(lan)):\n",
        "    print(\"Target language: \"+lan_str[i])\n",
        "    for j in range(len(lan)):\n",
        "        print(\"Source language: \"+lan_str[j])\n",
        "        for k in range(len(epoch_l)):\n",
        "            print(\"For Epoch :\"+str(epoch_l[k]))\n",
        "            reset_random_seeds()\n",
        "            # 1. Direct results\n",
        " #           filename='Single-L3-multi-lingual/results/CNN-digit-mfcc-T3-E'+str(epoch_l[k])+'-S'+lan[j]+'-D'+lan[i]+'.csv'\n",
        "            ##track log\n",
        "#            history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
        "            # Load direct models\n",
        "            model_load_test = keras.models.load_model(\"Single-L3-multi-lingual/CNN-digit-mfcc-T3-E\"+str(epoch_l[k])+\"-S\"+lan[j])\n",
        "            \n",
        "            # 2. Reverse results\n",
        "#           filename='Single-L3-multi-lingual/results-reverse/CNN-digit-mfcc-T3-E'+str(epoch_l[k])+'-S'+lan[j]+'-D'+lan[i]+'.csv'\n",
        "            ##track log\n",
        "#            history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
        "            # Load reverse models\n",
        "#            model_load_test = keras.models.load_model(\"Single-L3-multi-lingual/reverse/CNN-digit-mfcc-T3-E\"+str(epoch_l[k])+\"-S\"+lan[j])\n",
        "\n",
        "            # Evaluate the target languages\n",
        "            history=model_load_test.fit(source_tr[i],target_tr[i],validation_data=(source_te[i],target_te[i]),batch_size=32,epochs=30,verbose=1)  #,,callbacks=[history_logger]\n",
        "            # 1. Direct saved model\n",
        "#           model_load_test.save('Single-L3-multi-lingual/results/CNN-digit-mfcc-T3-E'+str(epoch_l[k])+'-S'+lan[j]+'-D'+lan[i])\n",
        "            # 2. reverse saved model\n",
        "#           model_load_test.save('Single-L3-multi-lingual/results-reverse/CNN-digit-mfcc-T3-E'+str(epoch_l[k])+'-S'+lan[j]+'-D'+lan[i])\n",
        "            \n",
        "#tf.reset_default_graph()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target language: Swahili\n",
            "Source language: Swahili\n",
            "For Epoch :1\n",
            "WARNING:tensorflow:From C:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From C:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "388/388 [==============================] - 2s 5ms/sample - loss: 2.3369 - acc: 0.0954\n",
            "Accuracy on test set is: 0.09536082297563553\n",
            "Train on 1552 samples, validate on 388 samples\n",
            "Epoch 1/30\n",
            "1552/1552 [==============================] - 26s 17ms/sample - loss: 1.9884 - acc: 0.2610 - val_loss: 2.4063 - val_acc: 0.0954\n",
            "Epoch 2/30\n",
            "1552/1552 [==============================] - 24s 16ms/sample - loss: 1.4568 - acc: 0.4659 - val_loss: 2.4440 - val_acc: 0.0954\n",
            "Epoch 3/30\n",
            "1552/1552 [==============================] - 26s 16ms/sample - loss: 1.1137 - acc: 0.5960 - val_loss: 2.3499 - val_acc: 0.1005\n",
            "Epoch 4/30\n",
            "1552/1552 [==============================] - 26s 17ms/sample - loss: 0.9088 - acc: 0.6765 - val_loss: 2.2768 - val_acc: 0.1005\n",
            "Epoch 5/30\n",
            "1552/1552 [==============================] - 25s 16ms/sample - loss: 0.8041 - acc: 0.7094 - val_loss: 2.1551 - val_acc: 0.1186\n",
            "Epoch 6/30\n",
            " 544/1552 [=========>....................] - ETA: 17s - loss: 0.6837 - acc: 0.7702"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-9-789df1d02707>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mtest_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_load_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_te\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_te\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy on test set is: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_load_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_te\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_te\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#,,callbacks=[history_logger]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;31m#            model_load_test.save('Single-L3-two/freeze/CNN-digit-mfcc-T3-E'+str(epoch_l[k])+'-S'+lan[j]+'-D'+lan[i])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mtest_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_load_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_te\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_te\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m   def evaluate(self,\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "T7aERc4yKNhD",
        "outputId": "c0a7add1-367c-4110-ce6e-7f4be9c6373e"
      },
      "source": [
        "# Proposed method\n",
        "epoch_l=[1,5,10,20,30]\n",
        "n=[4,7]  # Conv block sequence types A1-B2-B3 (4) and A1-A2-B3 (7) \n",
        "\n",
        "# Target languages\n",
        "s=['sw','en','gu'] \n",
        "# Source languages\n",
        "lan=['gu','sw']   #Swahili: ['en','gu']   English: ['gu','sw']  Gujarati: ['sw','en']\n",
        "# Select Target language index no 0,1,2 : Swahili, English, Gujarati\n",
        "indno=1\n",
        "# Language strings\n",
        "lan_str=[\"Swahili\",\"English\",\"Gujarati\"]\n",
        "# Training set\n",
        "source_tr=[X_train_sw,X_train_en,X_train_gu]  \n",
        "target_tr=[y_train_sw,y_train_en,y_train_gu]   \n",
        "# Test set\n",
        "source_te=[X_test_sw,X_test_en,X_test_gu]  \n",
        "target_te=[y_test_sw,y_test_en,y_test_gu]   \n",
        "\n",
        "\n",
        "for i in range(len(lan)):\n",
        "    print(\"Early layers language: \"+lan_str[i])\n",
        "    for j in range(len(epoch_l)):\n",
        "        print(\"Epoch :\"+str(epoch_l[j]))\n",
        "        for k in range(len(n)):\n",
        "            print(\"Number of layers: \"+str(n[k]))\n",
        "            reset_random_seeds()\n",
        "            \n",
        "            # Upload source 1 Pre-training weights = source data features\n",
        "            model_load_test = keras.models.load_model(\"Single-L3/CNN-digit-mfcc-T3-E\"+str(epoch_l[0])+\"-S\"+lan[i])\n",
        "            # Upload source 2 Pre-training weights = target data features\n",
        "            # Alternate epochs source numbers [1,5,10,20,30]\n",
        "            model_load_test_f = keras.models.load_model(\"Single-L3/CNN-digit-mfcc-T3-E\"+str(epoch_l[4])+\"-S\"+s[indno])  #CNN-digit-mfcc-g-sw_gu-d-16-\n",
        "            \n",
        "            # Mixing the weights of source 1 and source 2\n",
        "            # Select Conv block sequence types A1-B2-B3 and A1-A2-B3 \n",
        "            for l1,l2 in zip(model_load_test_f.layers[0:n[k]],model_load_test.layers[0:n[k]]):  \n",
        "                l1.set_weights(l2.get_weights())   # Mixing the weights\n",
        "      \n",
        " #           filename='Results_Single-Mix-gu/1/CNN-digit-mfcc-T3-E'+str(epoch_l[j])+'-S'+lan[i]+'gu-'+str(n[k])+'-Dgu.csv'\n",
        "            #track log\n",
        "#           history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\n",
        "            # The Target Gujarati training and Evaluating. \n",
        "            history=model_load_test_f.fit(source_tr[indno],target_tr[indno],validation_data=(source_te[indno],target_te[indno]),batch_size=32,epochs=30,verbose=1) #,callbacks=[history_logger]\n",
        "            "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early layers language: Swahili\n",
            "Epoch :1\n",
            "Number of layers: 4\n",
            "Train on 1552 samples, validate on 388 samples\n",
            "Epoch 1/30\n",
            "1552/1552 [==============================] - 26s 17ms/sample - loss: 0.2234 - acc: 0.9265 - val_loss: 6.5440 - val_acc: 0.1263\n",
            "Epoch 2/30\n",
            "1552/1552 [==============================] - 30s 19ms/sample - loss: 0.1463 - acc: 0.9510 - val_loss: 6.5873 - val_acc: 0.1753\n",
            "Epoch 3/30\n",
            "1552/1552 [==============================] - 40s 26ms/sample - loss: 0.1012 - acc: 0.9671 - val_loss: 6.5354 - val_acc: 0.1160\n",
            "Epoch 4/30\n",
            "1552/1552 [==============================] - 28s 18ms/sample - loss: 0.0608 - acc: 0.9807 - val_loss: 4.9575 - val_acc: 0.1366\n",
            "Epoch 5/30\n",
            "1552/1552 [==============================] - 31s 20ms/sample - loss: 0.0514 - acc: 0.9865 - val_loss: 5.0245 - val_acc: 0.1624\n",
            "Epoch 6/30\n",
            "1552/1552 [==============================] - 33s 21ms/sample - loss: 0.0498 - acc: 0.9878 - val_loss: 4.2244 - val_acc: 0.2216\n",
            "Epoch 7/30\n",
            "1552/1552 [==============================] - 28s 18ms/sample - loss: 0.0342 - acc: 0.9910 - val_loss: 3.5466 - val_acc: 0.3479\n",
            "Epoch 8/30\n",
            "1552/1552 [==============================] - 27s 18ms/sample - loss: 0.0286 - acc: 0.9929 - val_loss: 1.5740 - val_acc: 0.5619\n",
            "Epoch 9/30\n",
            "1552/1552 [==============================] - 27s 17ms/sample - loss: 0.0278 - acc: 0.9929 - val_loss: 1.6887 - val_acc: 0.5490\n",
            "Epoch 10/30\n",
            "1552/1552 [==============================] - 27s 17ms/sample - loss: 0.0505 - acc: 0.9852 - val_loss: 0.5081 - val_acc: 0.8351\n",
            "Epoch 11/30\n",
            "1088/1552 [====================>.........] - ETA: 7s - loss: 0.0290 - acc: 0.9945"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-15-d6d0e3106585>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m#           history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;31m# The Target Gujarati training and Evaluating.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_load_test_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindno\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindno\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_te\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindno\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_te\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindno\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#,callbacks=[history_logger]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m   def evaluate(self,\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4cn06AfKNhE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}